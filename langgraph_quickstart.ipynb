{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgR0DsKGasmHoN8qyiotfb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GenAIUnplugged/langGraph_series/blob/main/langgraph_quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYyISmR7pNR-",
        "outputId": "52684a9f-07d0-493f-efc8-602d0753ce7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.16-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.56)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.25)\n",
            "Requirement already satisfied: langgraph-prebuilt>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.66)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.4)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.39)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langchain-core>=0.1 (from langgraph)\n",
            "  Downloading langchain_core-0.3.59-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.76.2)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.13.2)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Downloading langchain_openai-0.3.16-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.59-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.7/437.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-core, langchain-openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.56\n",
            "    Uninstalling langchain-core-0.3.56:\n",
            "      Successfully uninstalled langchain-core-0.3.56\n",
            "Successfully installed langchain-core-0.3.59 langchain-openai-0.3.16 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install langgraph langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "tGIURYCRpwfG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, MessagesState, START\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.messages import HumanMessage"
      ],
      "metadata": {
        "id": "oS8AayKUp_Xb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create an agent using create_react_agent"
      ],
      "metadata": {
        "id": "ZTnhMDv9sIt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get weather for a given city.\"\"\"\n",
        "    return f\"It's always sunny in {city}!\"\n",
        "\n",
        "agent = create_react_agent(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    tools=[get_weather],\n",
        "    prompt=\"You are a helpful assistant\"\n",
        ")\n",
        "\n",
        "# Run the agent\n",
        "agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCTzSKb2pTCm",
        "outputId": "03fd5a4e-2181-4465-b7ea-6ba29f38d158"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='93a1ed4e-24b7-48ca-b844-d21aec5cc633'),\n",
              "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_u1Ws5TqbcrvJSgCzIObKEvF8', 'function': {'arguments': '{\"city\":\"San Francisco\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 56, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BW1Zjql2vvhKipwqdry2dAdaZIYNt', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--20b07a68-6d62-4932-92b7-bf188bc734bb-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_u1Ws5TqbcrvJSgCzIObKEvF8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 56, 'output_tokens': 16, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', id='570105d8-f11a-4f19-84df-5c7aa0b80cf6', tool_call_id='call_u1Ws5TqbcrvJSgCzIObKEvF8'),\n",
              "  AIMessage(content='The weather in San Francisco is currently sunny.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 86, 'total_tokens': 97, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BW1ZjRjzSF5EVZVuaQ15jS2rX3gSI', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--cf995732-bc7f-4870-8472-3656a9efe9a8-0', usage_metadata={'input_tokens': 86, 'output_tokens': 11, 'total_tokens': 97, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Configure an LLM using init_chat_model"
      ],
      "metadata": {
        "id": "fWInWfzKsNDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "model = init_chat_model(\n",
        "    \"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "agent = create_react_agent(\n",
        "    model=model,\n",
        "    tools=[get_weather],\n",
        ")"
      ],
      "metadata": {
        "id": "UsDZga8OqOBl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Add a custom prompt : Static: A string is interpreted as a system message."
      ],
      "metadata": {
        "id": "tEymu5TssSuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "agent = create_react_agent(\n",
        "    model=model,\n",
        "    tools=[get_weather],\n",
        "    # A static prompt that never changes\n",
        "    prompt=\"Never answer questions about the weather.\"\n",
        ")\n",
        "\n",
        "agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT5pTvuiqkzm",
        "outputId": "edc3912a-d815-49a5-d104-fc33875cc251"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='363ff17c-47bf-454a-b13a-d076f543938d'),\n",
              "  AIMessage(content=\"I'm unable to provide information about the weather. However, you can check a reliable weather website or app for the latest updates.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 57, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_dbaca60df0', 'id': 'chatcmpl-BW1cuHQBKXqb2LEs9yG1WYWgEppt6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--6decdd5e-b8bf-42fb-9843-02bf8cfc4902-0', usage_metadata={'input_tokens': 57, 'output_tokens': 27, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Dynamic: A list of messages generated at runtime, based on input or configuration."
      ],
      "metadata": {
        "id": "rlBnyymusdF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AnyMessage\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]:\n",
        "    user_name = config[\"configurable\"].get(\"user_name\")\n",
        "    system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\"\n",
        "    return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
        "\n",
        "agent = create_react_agent(\n",
        "    model=model,\n",
        "    tools=[get_weather],\n",
        "    prompt=prompt\n",
        ")\n",
        "\n",
        "agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n",
        "    config={\"configurable\": {\"user_name\": \"John Smith\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9vWHBnErBd-",
        "outputId": "ad93db45-21b2-4a4b-c295-cfb49ef73887"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='e660d83d-739c-4846-9a8f-f86b4af2dc47'),\n",
              "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Eukx5efjqs85B2kklYrwaiA3', 'function': {'arguments': '{\"city\":\"San Francisco\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 63, 'total_tokens': 79, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BW1ds7gPrUQk5i6yhxFIOOHDSw2CO', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--f15afe7d-a7a3-4d5a-a36b-be5d635b7370-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_Eukx5efjqs85B2kklYrwaiA3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 63, 'output_tokens': 16, 'total_tokens': 79, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', id='1d61980f-54cd-437a-8f6f-26f760877aa6', tool_call_id='call_Eukx5efjqs85B2kklYrwaiA3'),\n",
              "  AIMessage(content='The weather in San Francisco is always sunny! If you need more specific details or a forecast, just let me know, John Smith!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 93, 'total_tokens': 122, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BW1dsbThqfwQkcGzx97E5HYzQyas8', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--34dad369-ec57-4b72-825c-ee626ece124f-0', usage_metadata={'input_tokens': 93, 'output_tokens': 29, 'total_tokens': 122, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Add memory \\\n",
        "To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id — a unique identifier for the conversation (session):"
      ],
      "metadata": {
        "id": "1F2GEx-Cshgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "agent = create_react_agent(\n",
        "    model=model,\n",
        "    tools=[get_weather],\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "# Run the agent\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "sf_response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n",
        "    config\n",
        ")\n",
        "ny_response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]},\n",
        "    config\n",
        ")"
      ],
      "metadata": {
        "id": "EF7ws42xrNKl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Configure Structured Output \\\n",
        "To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. \\\n",
        "Structured output requires an additional call to the LLM to format the response according to the schema."
      ],
      "metadata": {
        "id": "ytO4Ir5estBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "class WeatherResponse(BaseModel):\n",
        "    conditions: str\n",
        "\n",
        "agent = create_react_agent(\n",
        "    model=model,\n",
        "    tools=[get_weather],\n",
        "    response_format=WeatherResponse\n",
        ")\n",
        "\n",
        "response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
        ")\n",
        "\n",
        "response[\"structured_response\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpFvwLk2r6Ie",
        "outputId": "e11d4e54-2001-4b89-821c-64fa9c59217f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WeatherResponse(conditions=\"It's always sunny in San Francisco!\")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AdgcWI-fsBhV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}